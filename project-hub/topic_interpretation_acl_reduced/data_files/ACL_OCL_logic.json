{"id": "200", "intruder_id": "8", "intruder_term": "dictionary", "labels": ["inference", "dictionary", "entailment", "logic", "semantics", "logical"], "text": "logical entailment semantics inference logic interpretation scope hypothesis predicate true predicates variable negation rule reasoning proof expression formula formal forms nli expressions variables operator premise", "coherence": "0.16545221482527392", "color": "Purple"}
{"id": "38", "intruder_id": "48", "intruder_term": "clause", "labels": ["network", "layer", "clause", "neural", "embedding", "lstm"], "text": "neural layer network lstm embedding embeddings hidden encoder networks layers architecture deep cnn rnn mechanism prediction vectors decoder recurrent loss memory convolutional encoding softmax matrix", "coherence": "0.4626473212748749", "color": "Borwn"}
{"id": "32", "intruder_id": "44", "intruder_term": "discourse", "labels": ["parser", "dependency", "parsing", "discourse", "parse", "treebank"], "text": "dependency parsing parser parse treebank tree head parsers trees dependencies structures constituent treebanks parses syntax gold arc pos penn transition attachment projective constituency constituents ccg", "coherence": "0.3548683873789557", "color": "Gray"}
{"id": "24", "intruder_id": "31", "intruder_term": "hindi", "labels": ["token", "bert", "hindi", "tuning", "transformer", "fine"], "text": "bert fine transformer tuning token shot pretrained loss roberta encoder devlin layer layers prediction pretraining downstream tuned masked embeddings transfer samples appendix batch span transformers", "coherence": "0.3531004636007982", "color": "Blue"}
{"id": "500", "intruder_id": "12", "intruder_term": "objective", "labels": ["tweets", "objective", "twitter", "users", "user", "social"], "text": "tweets social twitter user users media tweet posts comments messages post message online comment detection people hashtags thread email reddit forum emoji community privacy author", "coherence": "0.3478196271104164", "color": "Green"}
