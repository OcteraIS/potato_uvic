{"Interpretation-of-Topics-(Computational-Linguistics)-introduction.html": {"id": "Interpretation-of-Topics-(Computational-Linguistics)-introduction.html", "text": "introduction", "displayed_text": "introduction", "original_text": "introduction"}, "Interpretation-of-Topics-(Computational-Linguistics)-instructions.html": {"id": "Interpretation-of-Topics-(Computational-Linguistics)-instructions.html", "text": "instructions", "displayed_text": "instructions", "original_text": "instructions"}, "Interpretation-of-Topics-(Computational-Linguistics)-demographics.html": {"id": "Interpretation-of-Topics-(Computational-Linguistics)-demographics.html", "text": "demographics", "displayed_text": "demographics", "original_text": "demographics"}, "Interpretation-of-Topics-(Computational-Linguistics)-consent.html": {"id": "Interpretation-of-Topics-(Computational-Linguistics)-consent.html", "text": "consent", "displayed_text": "consent", "original_text": "consent"}, "24": {"id": "24", "intruder_id": "31", "intruder_term": "hindi", "labels": ["token", "bert", "hindi", "tuning", "transformer", "fine"], "text": "bert fine transformer tuning token shot pretrained loss roberta encoder devlin layer layers prediction pretraining downstream tuned masked embeddings transfer samples appendix batch span transformers", "coherence": "0.3531004636007982", "color": "Blue", "displayed_text": "bert<br>fine<br>transformer<br>tuning<br>token<br>shot<br>pretrained<br>loss<br>roberta<br>encoder<br>devlin<br>layer<br>layers<br>prediction<br>pretraining<br>downstream<br>tuned<br>masked<br>embeddings<br>transfer<br>samples<br>appendix<br>batch<br>span<br>transformers", "original_text": "bert fine transformer tuning token shot pretrained loss roberta encoder devlin layer layers prediction pretraining downstream tuned masked embeddings transfer samples appendix batch span transformers"}, "500": {"id": "500", "intruder_id": "12", "intruder_term": "objective", "labels": ["tweets", "objective", "twitter", "users", "user", "social"], "text": "tweets social twitter user users media tweet posts comments messages post message online comment detection people hashtags thread email reddit forum emoji community privacy author", "coherence": "0.3478196271104164", "color": "Green", "displayed_text": "tweets<br>social<br>twitter<br>user<br>users<br>media<br>tweet<br>posts<br>comments<br>messages<br>post<br>message<br>online<br>comment<br>detection<br>people<br>hashtags<br>thread<br>email<br>reddit<br>forum<br>emoji<br>community<br>privacy<br>author", "original_text": "tweets social twitter user users media tweet posts comments messages post message online comment detection people hashtags thread email reddit forum emoji community privacy author"}, "200": {"id": "200", "intruder_id": "8", "intruder_term": "dictionary", "labels": ["inference", "dictionary", "entailment", "logic", "semantics", "logical"], "text": "logical entailment semantics inference logic interpretation scope hypothesis predicate true predicates variable negation rule reasoning proof expression formula formal forms nli expressions variables operator premise", "coherence": "0.16545221482527392", "color": "Purple", "displayed_text": "logical<br>entailment<br>semantics<br>inference<br>logic<br>interpretation<br>scope<br>hypothesis<br>predicate<br>true<br>predicates<br>variable<br>negation<br>rule<br>reasoning<br>proof<br>expression<br>formula<br>formal<br>forms<br>nli<br>expressions<br>variables<br>operator<br>premise", "original_text": "logical entailment semantics inference logic interpretation scope hypothesis predicate true predicates variable negation rule reasoning proof expression formula formal forms nli expressions variables operator premise"}, "38": {"id": "38", "intruder_id": "48", "intruder_term": "clause", "labels": ["network", "layer", "clause", "neural", "embedding", "lstm"], "text": "neural layer network lstm embedding embeddings hidden encoder networks layers architecture deep cnn rnn mechanism prediction vectors decoder recurrent loss memory convolutional encoding softmax matrix", "coherence": "0.4626473212748749", "color": "Borwn", "displayed_text": "neural<br>layer<br>network<br>lstm<br>embedding<br>embeddings<br>hidden<br>encoder<br>networks<br>layers<br>architecture<br>deep<br>cnn<br>rnn<br>mechanism<br>prediction<br>vectors<br>decoder<br>recurrent<br>loss<br>memory<br>convolutional<br>encoding<br>softmax<br>matrix", "original_text": "neural layer network lstm embedding embeddings hidden encoder networks layers architecture deep cnn rnn mechanism prediction vectors decoder recurrent loss memory convolutional encoding softmax matrix"}, "32": {"id": "32", "intruder_id": "44", "intruder_term": "discourse", "labels": ["parser", "dependency", "parsing", "discourse", "parse", "treebank"], "text": "dependency parsing parser parse treebank tree head parsers trees dependencies structures constituent treebanks parses syntax gold arc pos penn transition attachment projective constituency constituents ccg", "coherence": "0.3548683873789557", "color": "Gray", "displayed_text": "dependency<br>parsing<br>parser<br>parse<br>treebank<br>tree<br>head<br>parsers<br>trees<br>dependencies<br>structures<br>constituent<br>treebanks<br>parses<br>syntax<br>gold<br>arc<br>pos<br>penn<br>transition<br>attachment<br>projective<br>constituency<br>constituents<br>ccg", "original_text": "dependency parsing parser parse treebank tree head parsers trees dependencies structures constituent treebanks parses syntax gold arc pos penn transition attachment projective constituency constituents ccg"}, "Interpretation-of-Topics-(Computational-Linguistics)-end.html": {"id": "Interpretation-of-Topics-(Computational-Linguistics)-end.html", "text": "end", "displayed_text": "end", "original_text": "end"}}